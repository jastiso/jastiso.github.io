<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
<div class="header">
  <h2>Blog Name</h2>
</div>

<div class="row">
  <div class="leftcolumn">
    <div class="card">
      <h2>Double/debiased machine learning</h2>
      <h5>Light- to medium-math exaplaination of the method with tutorials, June 10, 2021</h5>
      <img src="dml/dallerhinos.png" class="fakeimg" style="height:200px;" alt="Image of two rhinos, each with thie rown laptop, in the style of Henri Rausseau (curtosy of DALLE mini)"></img>
      <p>

        <h3>What is double machine learning</h3>
        Double machine learning is actually a way to esetimate causal effects in large, complex data that have a specific causal structure. Previously many causal 
        modeling methods relied on assuming a specific form of the data rather than learning it, and didn't allow with data with (DEFINITION OF COMPLEXITY), making them less useful in
         many modern data settings. DML allows these two to coexist, which is the source of much of the hype in economics. When I was trying to learn more about the method, I found that there weren't as many resources out there as I had hoped and that most of the resources that were out there had a very theoretical approach. I wanted to create a resource that explained the theory at a higher level, and had a larger emphasis on code based explainations. This is the info I wish I had when I was learning. 

        
        <h4>the problem</h4>
        Similarly, clinical machine learning projects have made a major push towards building risk or diagnostic models. Less attention has been devoted to using
        learning to suggest treatments or interventions. I think it's worth testing to waters to see if this can be done responsibly. DML presents one path forward. 
        I had mentioned previously that DML assumes a particular causal structure in your data. We can illustrate that struvture like this:
        
        <div class="fakeimg">Image of graph</div>
        
        Here, Y is some clinical outcome of interest (risk of disease, probability of diagnosis, etc.). T is some treatment or intervention. X is all the
         demographic, medical features of the patients. This diagram is illustrating that the treatment will influence the risk of disease, and that features of the
         patient will influence both the risk and which/whether treatments will be given. Broadly, the goal is to estimate the size of the arrow connecting T and Y
          (referred to now as the treatment effect). Specifically, we want that estimation to:
        <ol>
              <li> be accurate with a lot of data (might seem obvious, but this is harder than it sounds)</li>
              <li>come with confidence intervals</li>
              <li> not make strict assumptions about the form of the data (leverage machine learning)</li>
        </ol>
         
        
        <h4>DML's solution</h4>
        These points have historically been hard to acheive because methods for 'good' causal estimates typically do not give us point 3, and methods of machine
         learning typically do not give us point 1 (and sometimes 2). Machine learning models do not give good causal estimates for 2 reasons:
         <ol>
            <li>Regularization, necessary for fitting complex data, induces a bias. To reduce overfitting, analysts using machine learning methods often use 
                regularization. If you're familiar with LASSO or RIDGE regression, this is a classic example of using regularization. However, this necessarily 
                increases the bias of estimates.</li>
            <li>Despite our best efforts, machine learning tends to overfit data, further biasing results.
                DML can remove those two sources of bias and give us an estimate of the treatment effect and all of the extra points outlined above. At a high level, 
                these biases are alleviated by fitting two separate machine learning models (thus the name) to estimate the effect of X on Y and T, and then solving for
                theta using the residuals of those estimates (more details on this below). Additionally, there are now some pretty good packages implementing DML 
                in python that play nicely with scikit learn. All together making it a desireable new method for applied scientists, and motivating me to give it a try. </li>
         </ol>
        
        <h4>caveats and alternatives</h4>
        Like all methods, DML comes with important assumptions and caveats. 
        Assumptions (most of these are true of many causal methods):
        <ol>
            <li>Consistency - An individual's potential outcome under thier observed exposure history is precisely their observed outcome.</li>
            <li>Positivity - Everyone in the study has some probability of receiving treatment</li>
            <li>You are recording all variables that influence Y and T in X. I think this is the most fraught assumption in medical contexts (REF).</li>
        </ol>

        Caveats:
        <ol>
            <li> categorical treatment - at the moment, there isn't a way to use DML for a categorical treatment variable that also provides confidence intervals. Other methods, such as doubly robust learning, might be better suited here.</li>
            <li> biased data classes - DML is known to be biased in cases where one outcome is extremely rare (though it is less biased than many other methods). Over/undersampling of the data might be helpful in these cases.</li>
        </ol>
        
        Alternatives
        <ol>
            <li>Doubly robust learning</li>
            <li>TLME</li>
        </ol>
        
        <h3>How DML allows causal inference and machine learning to mix </h3>
        We're now going to describe the method in more detail than the above summary. The goal here is to hit the major points of the DML paper (REF) restructured
        for a more applied audience. I found these points difficult to work through on my own (thus the blog post). However if you're satisfied with the above description,
        skip to the code example.

        
        <h4>direct method</h4>
        To formalize the problems and solutions discussed above, we're going to have to be more mathematically precise with our definitions. We're going to start by
        defining a specific formula for generating data. 
        
        $$ {Y = T\theta_{0} + g_{0}(X) + U, E[U | X,T] = 0]} $$
        $$ {T = m_{0}(X) + V, E[V | X] = 0} $$
        
        Let's walk through the terms:
        <ul>
            <li>X the features</li>
            <li> Y the outcome</li>
            <li>T the treatment (it can be binary, continuous, or categorical)</li>
            <li> g(x) some mappong of x to y</li>
            <li> m(x) some mapping of x to t</li>
            <li> theta - the treatment effect</li>
            <li> the noise, which cancels out on average</li>
        </ul>
        
        This equation is essentially formalizing the graph we had displayed earlier.
        
        <div class="fakeimg">Image of graph</div>
        
        These equations are a useful example because they give us a specific functional form for how T affects Y (T * theta). Since this relationship is linear,
        it makes some of the math a little bit nicer. In the end, we want DML to work for more than just this specific situation, but this definition is useful for now. 
        If we were to code up these relationships in python, it would look something like this. Note that to code this up we have to pick a specific g_0(X) and m_0(X). It could be whatever you want, but here we're using some exponential sums of the first few columns of X (I picked this because that's what the original paper does).
        
        <pre><code class="python"> 
        from scipy.linalg import toeplitz
        # pick any value for theta_0
        theta = -0.4

        # define a function for generating data
        def get_data(n, n_x):
        """
        Inputs:
        n       the number of observations to simulate
        n_x     the number of columns of X to simulate
        """
            cov_mat = toeplitz([np.power(0.7, k) for k in range(n_x)])
            x = np.random.multivariate_normal(np.zeros(n_x), cov_mat, size=[n, ])
        
            u = np.random.standard_normal(size=[n, ])
            v = np.random.standard_normal(size=[n, ])
            m0 = x[:, 0] + np.divide(np.exp(x[:, 2]), 1 + np.exp(x[:, 2]))
            t = m0 + u
            g0 =  np.divide(np.exp(x[:, 0]), 1 + np.exp(x[:, 0])) + x[:, 2]
            y = theta * t + g0 + v
            
            return x, y, t        
        </code>></pre>
        
        Let's imaging you haven't read this blog post and you given some X,T, and Y data, as well as the data generating equations above. You're then asked to estimate what theta is. They tell you they know the answer, but want to see if you can find it on your own. 
        If you're like me, your first instinct might be to move some terma around in the equation to isolate theta, and then try to estimate all the missing variables with machine learning.  
        
        $$ EQUATION $$
        
        This is a little tricky because g0(X) is not the influence of X on Y, its the influence of X on the part of Y that isnt influenced by T. Therefore we have to do this iteratively
        <ol>
            <li>Get an initiail estimate of theta by solving an intermediate machine problem (l0(X)) that predicts Y from X.</li>
            <li>Subtract T from that initial estimate to fit g0</li> 
            <li>Regress T from g0 to get theta.</li>
        </ol>
         
        In code, the direct method would look like this.

        <pre><code class="python"> 
        import numpy as np
        from sklearn.ensemble import RandomForestRegressor
        import matplotlib.pyplot as plt

        # initializa structure for theta_0- - theta
        errors = []
        for i in range(n_sim):
            # get data
            x, y, t = get_data(n, n_x)
            
            # this will be our model for predicting Y from X
            ml_l = RandomForestRegressor()
            # this will be our model for predicting Y - T*theta from X
            ml_g = RandomForestRegressor()
        
            ml_l.fit(x,y)
            l_hat = ml_l.predict(x)
            
            # initial guess for theta
            u_hat = y - l_hat
            psi_a = -np.multiply(t, t)
            psi_b = np.multiply(t, u_hat)
            theta_init = -np.mean(psi_b) / np.mean(psi_a)
            
            # get estimate for g0
            ml_g.fit(x, y - t*theta_init)
            g_hat = ml_g.predict(x)
            
            # compute residuals
            u_hat = y - g_hat
            psi_a = -np.multiply(t, t)
            psi_b = np.multiply(t, u_hat)
            
            # get estimate and SE
            theta_hat = -np.mean(psi_b) / np.mean(psi_a)
            psi = psi_a * theta_hat + psi_b
            err = theta_hat - theta
            J = np.mean(psi_a)
            sigma2_hat = 1 / len(y) * np.mean(np.power(psi, 2)) / np.power(J, 2)
            err = err/np.sqrt(sigma2_hat)
            errors.append(err)
            
        plt.figure()
        plt.hist(errors)
        _, y_max = plt.ylim()
        plt.plot([0,0], [0,y_max], 'r')
        plt.show()
        </code>></pre>
         
         <div class="fakeimg">Image of histogram</div>
        
         Note that you could use whatever machine model you want, it doesn't have to be a random forest. In this example, it should be anything that can estimate exponential functions (since that's the form we picked for our data). Now we can see how well we did! If our estimate is good, we would expect the normalized difference between our estimate of theta, and the real theta to
         be centered on 0.

        This histogram shows that is not the case. Our estimate is way off and centered on a positive value. What went wrong?
        
        At a high level, what went wrong is that we did not explicitly model the effect of X on T. That influence is biasing our estimate in a few ways. 
        Illustrating this explicitly is where our partially linear data generating process comes in handy. We can write out an equation for the error in our estimate.
        As a reminder, the goal here would be for the left hand side to converge to 0 as we get more data.

         <h4>regularization bias</h4>
        
        $$ \sqrt{n}(\hat{\theta_0} - \theta_0) = (\frac{1}{n}\sum_{i\in I}^nT_{i}^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nT_{i}U_{i} + (\frac{1}{n}\sum_{i\in I}^nT_{i}^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nT_{i}(g_0(X_i) - \hat{g_0}(X_i))$$
        
        <ul>
            <li>LHS</li>
            <li>Noise cancels out on average, so this term is a very small number, divided by a big number. Essentially 0</li>
            <li>This term is where the problem is. Our estimate error is never going to be 0, and won;t quite be cenmtered on 0. This because of the deal we make as 
                data scientists working with complex data. Reduce the varaince (overfitting) of our machine learning model, we induce some bias in our estimate (often through 
                regularization). Additionally, T depends on X, and therefore also will not converge to 0. Because of this, g-g_hat times T will be small, but not zero. 
                It will be divided by a large number, and will converge to 0 eventually, but too slowly to be practical. </li>
        </ul>
        
        We have to remove the effect of X on T to circumvent this bias. We can do this in three steps:
        <ol>
            <li>Estimate T from X using ML model of choice (different from the direct method!)</li>
            <li> Estimate Y from X using ML model of choice</li>
            <li> Regress the residuals of each model onto eachother to get theta</li>
        </ol>

        We can write out a new error equation like so:
        
        $$ { \sqrt{n}(\hat{\theta_0} - \theta_0) = (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nU_iV_i + (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^n(\hat{m_0}(X_i) - m_0(X_i))(\hat{g_0}(X_i) - g_0(X_i)) + ... } $$
       
        <ul>
            <li>LHS</li>
            <li>Noise cancels out on average, so this term is a very small number, divided by a big number. Essentially 0</li>
            <li>Now we have two small, non-0 numbers multiplied by eachother, divided by a large number. This will converge to 0 much more quickly than before </li>
            <li> ... this method adds a new term that we're going to ignore for now. But it comes back later!</li>
        </ul>
        
        In code, this new process looks like this:
        <pre><code class="python"> 
        
        errors = []
        for i in range(n_sim):
            # get data
            x, y, t = get_data(n, n_x)
            
            # model for predicting Y from X
            ml_l = RandomForestRegressor()
            # model for predicting T from X - new to the regularized version!
            ml_m = RandomForestRegressor()
            # model for predicting Y - T|X*theta from X
            ml_g = RandomForestRegressor()
        
            ml_l.fit(x,y)
            ml_m.fit(x,t)
            l_hat = ml_l.predict(x)
            m_hat = ml_m.predict(x)
            
            # initial guess for theta
            u_hat = y - 
            # this is the part that's different
            v_hat = t - m_hat
            psi_a = -np.multiply(v_hat, v_hat)
            psi_b = np.multiply(v_hat, u_hat)
            theta_init = -np.mean(psi_b) / np.mean(psi_a)
            
            # get estimate for G
            ml_g.fit(x, y - t*theta_init)
            g_hat = ml_g.predict(x)
            
            # compute residuals
            u_hat = y - g_hat
            psi_a = -np.multiply(v_hat, v_hat)
            psi_b = np.multiply(v_hat, u_hat)
            
            theta_hat = -np.mean(psi_b) / np.mean(psi_a)
            psi = psi_a * theta_hat + psi_b
            err = theta_hat - theta
            J = np.mean(psi_a)
            sigma2_hat = 1 / len(y) * np.mean(np.power(psi, 2)) / np.power(J, 2)
            err = err/np.sqrt(sigma2_hat)
            errors.append(err)
            
        plt.figure()
        plt.hist(errors)
        _, y_max = plt.ylim()
        plt.plot([0,0], [0,y_max], 'r')
        plt.show()   
        </code>></pre>

        And we have greatly reduced our bias!

        For this specific data generating process, we now have a way of estimating theta without regularization bias! However I mentioned earlier that we want to be 
        able to estimate more than only this process. In particular, step three involves linear regression, and only works in our partially linear example. How do
         we generalize the method of estimating theta?
         
         The least squares solution for linear relationships esentially finds the parameters for a line that minimizes the error between the predicted points on 
         the line, and the observed data. We can wrote this as the minimization of a cost function of our data and true parameters
         
         $$ \psi(W; \theta, \eta) = 0 $$
         
         This equation looks vary different but contains a lot of the same players as before:

         <ol>
             <li>W is the data (X,Y, and T)</li>
             <li>Theta is the true treatment effect</li>
             <li> psi is just some cost function. We are purposely not defining it because we want this to be a general solution, but you can think of it as any kind of 
                error minimization function</li>
             <li> eta is called the nuissance parameter, and here contains g and m</li>
         </ol>

         Solving minimization problems like these are often difficult and subject to noise. To assure we find a robust solution, we're going to add one other condition
          to our equation (called a moment condition)
          
        $$ { \delta_{\eta}E[\psi(W; \theta, \eta][\eta - \eta_0] = 0} $$
         
         Technically, this is a direction al Gateaux derivative. Practically, what this means is that we expect that the true value of theta would be robust to 
         small purturbations in the nuissance parameters. This has the benefit of giving use estimates that will be stable in the presence of small changes to our
         ML models.
         
         There are whole branches of mathematics dedidated to solving these tpyes of equations with moment conditions, and there is no single good solution. All the 
         different solutions are called 'DML estimators'. Rather than getting into any specific estimator here, we're just going to trust that they exist, and move on.
         We'll talk a little bit about a specific estimator in the code section.
         
         We now have a more generalizable set of steps
         <ol>
             <li> Estimate T from X</li>
             <li> Estimate Y from X</li>
             <li> Solve moment equation to get theta</li>
         </ol>
        
         <h4>overfitting bias</h4>
        We now have a generalizable solution to regulatization bias. Additionally, with the definition of a cost function, we have a method of evaluating our DML
         estimator and comparing different models. Specifically, we can find the model that gives smallest value for our moment condition. The specific value of
         this function is usually called a 'score' or 'Neyman orthogonality score', and the closer to 0 it is the better. We will use this value to perform model 
        selection in the example later.
        
        But first, it's time to revisit out error equation in the partially linear case.
        
        $$ { \sqrt{n}(\hat{\theta_0} - \theta_0) = (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nU_iV_i + (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^n(\hat{m_0}(X_i) - m_0(X_i))(\hat{g_0}(X_i) - g_0(X_i)) + \frac{1}{\sqrt{n}}\sum_{i \in I}^{n}V_i(\hat{g_0}(X_i) - g_0(X_i)) } $$
        
        We've discussed the first terms perviously, but I'm revealing the last term we had hidden previously.
        * If any overfitting is present in g_hat, the estimate will pick up some noise from the noise term. This will slow down the convergence of this new term to 0
        
        The solution to this bias is to fit g and m on a different set of data than the set used to estimate theta. Similar to how cross-validation avoids overfitting 
        during parameter selection, this method (called cross-fitting) avoids overfitting in out estimation of theta. This changes our DML steps slightly.
        
        <ol>
            <li>Split the data into K folds. For each fold:</li>
            <li> Estimate T from X using ML model of choice and fold K</li>
            <li> Estimate Y from X using ML model of choice and fold K</li>
            <li> Solve moment equation get theta using other sets of data</li>
            <li>Select theta estimate that gives the best solution over all splits.</li>
        </ol>

        In code, all this does is add a loop over folds:

        <pre><code class="python">
        from sklearn.model_selection import KFold
        # number of splits for cross fitting
        nSplit = 2
        errors = []
        for i in range(n_sim):
            # get data
            x, y, t = get_data(n, n_x)
            # cross fit
            kf = KFold(n_splits=nSplit)
            # save theta hats, and some variables for getting variation in theta_hat
            theta_hats = []
            sigmas = []
            for train_index, test_index in kf.split(x):
                x_train, x_test = x[train_index], x[test_index]
                y_train, y_test = y[train_index], y[test_index]
                t_train, t_test = t[train_index], t[test_index]
                
                ml_l = RandomForestRegressor()
                ml_m = RandomForestRegressor()
                ml_g = RandomForestRegressor()
        
                ml_l.fit(x_train,y_train)
                ml_m.fit(x_train,t_train)
                l_hat = ml_l.predict(x_test)
                m_hat = ml_m.predict(x_test)
                
                # initial guess for theta
                u_hat = y_test - l_hat
                v_hat = t_test - m_hat
                psi_a = -np.multiply(v_hat, v_hat)
                psi_b = np.multiply(v_hat, u_hat)
                theta_init = -np.mean(psi_b) / np.mean(psi_a)
        
                # get estimate for G
                ml_g.fit(x_train, y_train - t_train*theta_init)
                g_hat = ml_g.predict(x_test)
        
                # compute residuals
                u_hat = y_test - g_hat
        
                psi_a = -np.multiply(v_hat, v_hat)
                psi_b = np.multiply(v_hat, u_hat)
        
                theta_hat = -np.mean(psi_b) / np.mean(psi_a)
                theta_hats.append(theta_hat)
                psi = psi_a * theta_hat + psi_b
                sigma2_hat = 1 / len(y_test) * np.mean(np.power(psi, 2)) / np.power(J, 2)
                sigmas.append(sigma2_hat)
            err = np.mean(theta_hat) - theta
            err = err/np.sqrt(np.mean(sigma2_hat))
            errors.append(err)
            
        plt.figure()
        plt.hist(errors)
        _, y_max = plt.ylim()
        plt.plot([0,0], [0,y_max], 'r')
        plt.show()
        </code></pre>

        Using this process, we can correct the bias in our stimateion

        <div class="fakeimg">Image of graph</div>
        
        This is the DML method! Now I'll show you how to apply it.
         
        <h3>Example, with code</h3>
        We're now going to walk through an example application of DML to a simulated medical dataset. If you didn't read the above section, it will be usefull to check in with the steps listed above. For simplicity, we're going to use the same data generating process we discuessed above
        
        $$ {Y = T\theta_{0} + g_{0}(X) + U, E[U | X,T] = 0]} $$
        $$ {T = m_{0}(X) + V, E[V | X] = 0} $$
        
        As a reminder, X are our patient features, Y is a binary predictor of outcome or risk, and T is a binary indicator of treatment. Theta is the treatment effect, the term we're interested in. We can simulate this data in python using the same code as before. As a reminder:
        
        <pre><code class="python"> simulating data code </code>></pre>
        
        We're also going to split our data into folds for evaluation. It's important to remember that each training fold is going to be further split within the DML estimator, so we want to make sure there are enough observations in the fold to split further.
        
        <pre><code class="python"> data splitting code </code>></pre>
        
        The econML package has different 'estimator' functions that all take data, a Y model, and a T model as input. Therefore, we're going to start with step 2 listed above: estimating T (we'll do the data splitting process later). You can really do this however you want, we're just going to define a simple random forest model here. econML has the benefit of working with GridSearchCV object as well, so we're actually going to define not one speciifc model, but a set models with different parameterizations (if you aren't familiar with GridSearchCV, check out its docs).
        
        <pre><code class="python"> 
        # parameters for forest
        params = {
            'max_depth': [5, 10],
            'min_samples_leaf': [2, 4],
            'min_samples_split': [2, 4],
            'class_weight': ['balanced', None],
            'n_estimators':[400, 1000]
            }
        
        t_mdls = GridSearchCV(RandomForestClassifier(),
                                params,
                                cv=nFold,
                                verbose=1,
                                scoring='f1_macro')
        t_mdl = t_mdls.fit(X_train, np.ravel(y_train)).best_estimator_ 
        </code>></pre>

        You can evaluate these models any way you want to. We'll use ROC
        <pre><code class="python"> 
        plt.figure()

        pred = t_mdl.predict_proba(X_test)[:,1]
            
        fpr, tpr, _ = roc_curve(y_test,pred,drop_intermediate=False)
        roc_auc = roc_auc_score(y_test,pred)

        plt.plot(fpr, tpr,
                lw=lw, label='Risk: %0.2f' % roc_auc)
        plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Outcome ROC Curves')
        plt.legend(loc="best")
        plt.show() 
        </code>></pre>
        <div class="fakeimg">Image of graph</div>

        We can do exactly the same thing for the Y model. Let's use a gradient boosted classifier to mix things up.
        
        <pre><code class="python"> 
        # parameters for XGB
        params = {
            'max_depth': range (2, 10, 1),
            'n_estimators': range(60, 220, 40),
            'learning_rate': [0.1, 0.01, 0.05]
            }
        y_mdls = GridSearchCV(XGBClassifier(use_label_encoder=False,eval_metric='logloss', n_jobs=1),
                                params,cv=nFold,n_jobs=nJobs,verbose=1,scoring = 'f1_macro')
        
        y_mdl = y_mdls.fit(X_train_clean, np.ravel(y_train)).best_estimator_
        </code>></pre>

        <pre><code class="python"> 
        plt.figure()

        pred = y_mdl.predict_proba(X_test)[:,1]
            
        fpr, tpr, _ = roc_curve(y_test,pred,drop_intermediate=False)
        roc_auc = roc_auc_score(y_test,pred)

        plt.plot(fpr, tpr,
                lw=lw, label='Risk: %0.2f' % roc_auc)
        plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Outcome ROC Curves')
        plt.legend(loc="best")
        plt.show() 
        </code>></pre>
        <div class="fakeimg">Image of graph</div>
        
        We can now pick an estimator. econML has 3 main estimators that provide confidence intervals. SparseLinear and Linear estimator will only work if you have many more observations than variables (see this link for comparisons of different estimators). For my data, this was not the case, so left me with 1 option: CausalForest estimator (REF). Like random forest models, this estimator also has the benefit of being able to estimate non-linear treatment effects in a piece-wise fashion.
        
        <pre><code class="python"> 
        est = CausalForestDML(model_y=y_mdl, model_t=t_mdl, discrete_treatment=True, cv=3)
        est.fit(Y=y_train, T=T_train, X=X_train) 
        </code>></pre>
        
        This function takes care of the cross-fitting procedure! The argument XX defines how many folds to use for cross fitting. The default is 3, but the original paper recommends using 5 or 6 if possible.
        
        Also similar to RandomForestCLassifiers in scikit learn, the CausalForest estimator has many other parameters. If you're familiar with random forests many of these parameters will be familiar: the number of trees to include, the maximum depth of those trees, etc. The big exception in parameters between causal forests in econML and sklearn is that econML forest has no class weightnig option (XXX). This is because the causal forest method makes use of a specific weighting strategy already (REF). 
        
        Additionally, the econML estimator does not take the 'params' argument, and therefore can't be used as input into sklearns Grid or RandomSearchCV functions. However we can do our own hyperparameter selection by comparing the 'score' between different parameter choices and selecting the lowest one. For brevity, we're going to use a random sampling strategy for hyperparameter selection. This means that rather than trying every possible parameter combination, we're going to randomly select some and pick the best option, assuming that we will get similar performance in the long run
        
        <pre><code class="python"> 
        # save the scores so we can plot them
        no_score = pd.DataFrame(columns=['depth', 'min_leaf', 'min_split', 'n', 'score'])
        # parameters for causal forest
        est_params = {
            'max_depth': [5, 10, None],
            'min_samples_leaf': [5, 2],
            'min_samples_split': [10, 4],
            'n_estimators': [100, 500]
        }
        n_iter = 20  # number of random parameter sets
        skf = StratifiedKFold(n_splits=nFold)
        </code></pre>
        
        Now that we how our cross validation and parameters set up, we can find the best parameters
        
        <pre><code class="python">
        # randomly pick some parameters
        n = np.random.choice(est_params['n_estimators'])
        d = np.random.choice(est_params['max_depth'])
        l = np.random.choice(est_params['min_samples_leaf'])
        s = np.random.choice(est_params['min_samples_split'])

        # get the cate estimate
        est = CausalForestDML(model_y=y_mdl,
                            model_t=t_mdl,
                            discrete_treatment=True, 
                            verbose=0, 
                            max_depth=d,
                            min_samples_leaf=l,
                            min_samples_split=s,
                            n_estimators=n)

        # stratified cv
        for train_index, test_index in skf.split(x, y):
            X_htrain, X_htest = x[train_index], x[test_index]
            y_htrain, y_htest = y[train_index], y[test_index]
            T_htrain, T_htest = t[train_index], t[test_index]

            est.fit(Y=np.ravel(y_htrain), T=T_htrain, X=X_htrain)
            scores.append(est.score(y_htest, T_htest, X=X_htest))

            curr_dict = {'depth': [d],
                        'min_leaf': [l],
                        'min_split': [s],
                        'n': [n],
                        'score': [np.mean(scores)],
                        'score_sc': [np.std(scores)],
                        'med': [med]}
            no_score = pd.concat([hp_score, curr_dict])

        # pick the best parameters, and fit the model
        best_n = no_score.loc[no_score['score'] == no_score['score'].min(), 'n'].values[0]
        best_d = no_score.loc[no_score['score'] == no_score['score'].min(), 'depth'].values[0]
        best_msl = no_score.loc[no_score['score'] == no_score['score'].min(), 'min_leaf'].values[0]
        best_mss = no_score.loc[no_score['score'] == no_score['score'].min(), 'min_split'].values[0]
        best_est = CausalForestDML(model_y=y_mdl,
                                    model_t=t_mdl,
                                    discrete_treatment=True, verbose=0, cv=3,
                                    max_depth=best_d,
                                    min_samples_leaf=best_msl,
                                    min_samples_split=best_mss,
                                    n_estimators=best_n)
        best_est.fit(np.ravel(y_train), T_train, X=X_train)
        </code>></pre>
        
        The 'score' function (if you use the default option) tells you the value of the cost function defining Neyman orthogonality. Its essentially telling you how close you were to finding a theta value that minimizes error/maximizes liklihood, and it stable for perturbations to your two machine learning models. Satisfying these two conditions assures an unbiased estimate of theta. While this number is a bit harder to interpret than MSE you might be used to, for the purpose of model or parameter selection it is satisfactory to select the model that gives the lowest score.
        
        <pre><code class="python"> 
        # print scores for each medicine
        hp_score['param_set'] = ["_".join(x) for x in hp_score[['n', 'depth', 'min_leaf', 'min_split']].astype(str).values]
        sns.stripplot(data=hp_score,  y="score", hue="param_set", size=10)
        plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")
        </code>></pre>
        <div class="fakeimg">Image of graph</div>
        
        We can now fit the model using the parameters that gave the lowest score
        
        <pre><code class="python"> 
        xxx    
        </code>></pre>
        
        We can now look at the distribution for of CATE's for all individuals in the test set.
        
        <pre><code class="python"> 
        # plot
        n_bins = 10
        effect = est.effect(X=X_test_clean)
        plt.hist(effect, bins = n_bins, label=med,alpha=.8, color=colors[med_idx])
        plt.title('Effect Values')
        plt.xlabel('Effect')
        plt.ylabel('Count')    
        </code>></pre>
        <div class="fakeimg">Image of graph</div>

        We can also look at the CI for a single individual.

        <pre><code class="python"> 
        # get individual CATES
        patient_idx = np.random.randint(np.shape(X_test)[0])
                
        # get cate
        cate = mdl.effect(X_test[patient_idx:patient_idx+1,])[0]
        
        # get cate CI
        lb, ub = mdl_dict[name].effect_interval(X_test_clean[patient_idx:patient_idx+1,], alpha=0.05)
        
        # plot CATEs with CI for individual patients
        plt.figure(figsize=(8,6))
        plt.errorbar(1, cate, yerr=ci, 
                        fmt="o", ecolor='k', zorder=1)
        plt.tight_layout()
        </code>></pre>
        <div class="fakeimg">Image of graph</div>

        How'd we do? Since this is simulated data, we can see how well our estimated treament matches

        <pre><code class="python"> 
        # plot   
        </code>></pre>

        We did pretty well! As expected, given the simulated data. This concludes the extent of the code walkthrough. If you're still looking for more, check out the other resources I found useful below.
        
        <h3>Other resources</h3>
        <ul>
            <li>blogs (liking old twitter posts)</li>
            <li>slides</li>
            <li>econml tutorials</li>
        </ul>
        
        <h3>References</h3>
        <ul>
            <li>causal inferences issue</li>
            <li>DML paper</li>
            <li>GRF estimator</li>
        </ul>
       
        </p>
    </div>
    
 <!---   <div class="card">
      <h2>TITLE HEADING</h2>
      <h5>Title description, Sep 2, 2017</h5>
      <div class="fakeimg" style="height:200px;">Image</div>
      <p>Some text..</p>
    </div>
  </div>
  <div class="rightcolumn">
    <div class="card">
      <h3>Popular Post</h3>
      <div class="fakeimg">Image</div><br>
      <div class="fakeimg">Image</div><br>
      <div class="fakeimg">Image</div>
    </div>
    
  </div>
--->
</div>

