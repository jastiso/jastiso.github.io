<link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
<div class="row">
  <div class="leftcolumn">
    <div class="card">
      <h2>Double/debiased machine learning</h2>
      <h5>Light- to medium-math exaplaination of the method with tutorials, June 10, 2021</h5>
      <img src="dml/dml_bird.jpg" class="title_img" alt="A photograph of two wading birds, facing opposite directions. Taken with a vintage Nikon 300mm lens"></img>
      <p>

        <h3>What is double machine learning</h3>
        To me -- and potentially, <span href="https://youtu.be/eHOjmyoPCFU?t=110"> its creator </span>-- Double Machine Learning sounds like a trendy name you would give a method to try to sound impressive in developer spaces. Double (or debiased) machine learning is actually a way to esetimate causal effects in large, complex data that have a specific causal structure. Previously many causal 
        modeling methods relied on assuming a specific form of the data rather than learning it - namely that varialbes were normally distibuted and linearly related to eachother. Additionally, these methods don't allow for complex data, or data where the entropy of the parameter space increases with increasing observations (in otherwords - most large, modern datasets). Combined, these features made causal inference difficult to apply to realworld problems.
        Double machine learning (DML) allows causal inference to coexist with complex data with few assumptions, which has drummed up a lot of well deserved excitement about the method (primarily in economics). 
        When I was trying to learn more about the method, I found that there weren't as many resources out there as I had hoped and that most of the resources that were out there had a very theoretical approach. I wanted to create a resource that explained the theory at a higher level, and had a larger emphasis on code based explainations. This is the info I wish I had when I was learning. 
        This post focuses on understanding how the DML algorthim works. I hop later to do a post on its application, but for now there are some good tutorial <span href="https://econml.azurewebsites.net/spec/estimation/dml.html">here.</span>

        
        <h4>what clinical problems can DML be used for?</h4>
        The adoption of this methods aligns nicely with some new trends in clinical informatics (my current field). Clinical machine learning projects have made a major push towards building risk or diagnostic models. Less attention has been devoted to using
        learning to suggest treatments or interventions. DML presents one path forward. 
        I had mentioned previously that DML assumes a particular causal structure in your data, and therefore a particular type of problem. We can illustrate that struvture like this:
        
        <img src="dml/causal_diag.png" class="post_img" alt="A graph with three nodes, labeled X, Y, and T. There are arrows connecting X to Y and T, and an arrow connecting T to Y"></img>
        
        Here, Y is some clinical outcome of interest (risk of disease, probability of diagnosis, etc.). T is some treatment or intervention. X is all the
         demographic, medical features of the patients. This diagram is illustrating that the treatment will influence the risk of disease, and that features of the
         patient will influence both the risk and which/whether treatments will be given. Broadly, the goal is to estimate the size of the arrow connecting T and Y
          (referred to now as the treatment effect). Specifically, we want that estimation to:
        <ol>
              <li> be accurate with a lot of data (might seem obvious, but this is harder than it sounds)</li>
              <li>come with confidence intervals</li>
              <li> not make strict assumptions about the form of the data (leverage machine learning)</li>
        </ol>
         
        
        <h4>DML's solution</h4>
        These points have historically been hard to acheive because methods for 'good' causal estimates typically do not give us point 3, and methods of machine
         learning typically do not give us point 1 (and sometimes 2). Machine learning models do not give good causal estimates for 2 reasons:
         <ol>
            <li>Regularization, necessary for fitting complex data, induces a bias (think bias variance trade-ff). To reduce overfitting, analysts using machine learning methods often use 
                regularization. If you're familiar with LASSO or RIDGE regression, this is a classic example of using regularization. However, this necessarily 
                increases the bias of estimates.</li>
            <li>Despite our best efforts, machine learning tends to overfit data, further biasing results.
                DML can remove those two sources of bias and give us an estimate of the treatment effect and all of the extra points outlined above. At a high level, 
                these biases are alleviated by fitting two separate machine learning models (thus the name) to estimate the effect of X on Y and T, and then solving for
                theta using the residuals of those estimates (more details on this below). Additionally, there are now some pretty good packages implementing DML 
                in python that play nicely with scikit learn. All together making it a desireable new method for applied scientists, and motivating me to give it a try. </li>
         </ol>
        
        <h4>caveats and alternatives</h4>
        Like all methods, DML comes with important assumptions and caveats. 
        Assumptions (most of these are true of many causal methods):
        <ol>
            <li>Consistency - An individual's potential outcome under thier observed exposure history is precisely their observed outcome.</li>
            <li>Positivity - Everyone in the study has some probability of receiving treatment</li>
            <li>You are recording all variables that influence Y and T in X. I think this is the most fraught assumption in medical contexts (REF).</li>
        </ol>

        Caveats:
        <ol>
            <li> categorical treatment - at the moment, there isn't a way to use DML for a categorical treatment variable that also provides confidence intervals. Other methods, such as doubly robust learning, might be better suited here.</li>
            <li> biased data classes - DML is known to be biased in cases where one outcome is extremely rare (though it is less biased than many other methods). Over/undersampling of the data might be helpful in these cases.</li>
        </ol>
        
        Alternatives
        <ol>
            <li>Doubly robust learning</li>
            <li>TLME</li>
        </ol>
        
        <h3>How DML allows causal inference and machine learning to mix </h3>
        We're now going to describe the method in more detail than the above summary. The goal here is to hit the major points of the DML paper (REF) restructured
        for a more applied audience. I found these points difficult to work through on my own (thus the blog post). However if you're satisfied with the above description,
        skip to the code example.

        
        <h4>direct method</h4>
        To formalize the problems and solutions discussed above, we're going to have to be more mathematically precise with our definitions. We're going to start by
        defining a specific formula for generating data. 
        
        $$ {Y = T\theta_{0} + g_{0}(X) + U, E[U | X,T] = 0]} $$
        $$ {T = m_{0}(X) + V, E[V | X] = 0} $$
        
        Let's walk through the terms:
        <ul>
            <li> \(X\) the features</li>
            <li> \(Y\) the outcome</li>
            <li> \(T\) the treatment (it can be binary, continuous, or categorical)</li>
            <li> \(g_0(x)\) some mappong of x to y, excluding the effect of T and $\theta_0$</li>
            <li> \(m_0(x)\) some mapping of x to t</li>
            <li> \(\theta\) - the treatment effect. Here its a scalar, for simplicity, but this doesn't have to be the case</li>
            <li> \(U, V\) the noise, which cancels out on average</li>
        </ul>
        
        This equation is essentially formalizing the graph we had displayed earlier.
        
        <img src="dml/causal_diag.png" class="post_img" alt="A graph with three nodes, labeled X, Y, and T. There are arrows connecting X to Y and T, and an arrow connecting T to Y"></img>
        
        These equations are a useful example because they give us a specific functional form for how $T$ affects $Y$ ($T \times \theta_0$). Since this relationship is linear,
        it makes some of the math a little bit nicer. In the end, we want DML to work for more than just this specific situation, but this definition is useful for now. 
        If we were to code up these relationships in python, it would look something like this. Note that to code this up we have to pick a specific $g_0(X)$ and $m_0(X)$. It could be whatever you want, but here we're using some exponential sums of the first few columns of X (I picked this because that's what the original paper does).
        
        <pre><code class="python"> 
        from scipy.linalg import toeplitz
        # pick any value for theta_0
        theta = -0.4

        # define a function for generating data
        def get_data(n, n_x, theta):
        """
        partially linear data generating process
        
        Inputs:
        n       the number of observations to simulate
        n_x     the number of columns of X to simulate
        theta   a scalar value for theta
        """
            cov_mat = toeplitz([np.power(0.7, k) for k in range(n_x)])
            x = np.random.multivariate_normal(np.zeros(n_x), cov_mat, size=[n, ])
        
            u = np.random.standard_normal(size=[n, ])
            v = np.random.standard_normal(size=[n, ])
            m0 = x[:, 0] + np.divide(np.exp(x[:, 2]), 1 + np.exp(x[:, 2]))
            t = m0 + u
            g0 =  np.divide(np.exp(x[:, 0]), 1 + np.exp(x[:, 0])) + x[:, 2]
            y = theta * t + g0 + v
            
            return x, y, t        
        </code>></pre>
        
        Let's imagine you're given some X, T, and Y data by your emploter, as well as the data generating equations above. You're then asked to estimate what theta is. 
        One first attempt might be to build one machine learning model of \(T\theta_{0} + g_{0}(X)\) and \(g_0(X)\), then regress out \(T\) to solve for \(\theta_0\).  
        
        This is a little tricky because \(g_0(X)\) is not the influence of \(X\) on \(Y\), its the influence of \(X\) on the part of \(Y\) that isnt influenced by \(T \times \theta_0\). Therefore we have to do this iteratively: get an initial guess for \(\theta_0\) in order to estimate \(g_0(X)\); then use that estimate of \(g_0(X)\) to solve for \(\theta_0\).
        In code, the direct method would look like this:
        
        First, we'd simulate our data, and build our machine learning estimate of \(Y\) from \(T\theta_{0} + g_{0}(X)\) (we'll call this model \(l_0(X)\))
        <pre><code class="python"> 
        from sklearn.ensemble import RandomForestRegressor
    
        # get data
        x, y, t = get_data(n, n_x, theta)
            
        # this will be our model for predicting Y from X
        ml_l = RandomForestRegressor()
        ml_l.fit(x,y)
        </code>></pre>
        
        Note that you could use whatever machine model you want, it doesn't have to be a random forest. In this example, it should be anything that can estimate exponential functions (since that's the form we picked for our data generating function). Next we can take an initial guess for \(\theta_0\), and then fit our estimate of \(g_0(X)\)
        <pre><code class="python"> 
         # this will be our model for predicting Y - T*theta from X, or g0_hat
         ml_g = RandomForestRegressor()
            
         # initial guess for theta
         l_hat = ml_l.predict(x)
         psi_a = -np.multiply(t, t)
         psi_b = np.multiply(t, y - l_hat)
         theta_init = -np.mean(psi_b) / np.mean(psi_a)
            
         # get estimate for g0
         ml_g.fit(x, y - t*theta_init)
         g_hat = ml_g.predict(x)
         </code>></pre>

        Lastly, we can regress the effect of T our from our prediction
        <pre><code class="python"> 
        # compute residuals
        u_hat = y - g_hat
        psi_a = -np.multiply(t, t)
        psi_b = np.multiply(t, u_hat)
            
        # get estimate of theta and and SE
        theta_hat = -np.mean(psi_b) / np.mean(psi_a)
        psi = psi_a * theta_hat + psi_b
        err = theta_hat - theta
        J = np.mean(psi_a)
        sigma2_hat = 1 / len(y) * np.mean(np.power(psi, 2)) / np.power(J, 2)
        err = err/np.sqrt(sigma2_hat)
        </code>></pre>

        If we repeat this process 200 times, we can genereate a histogram of our error term and see how well we did.
         
         <img src="dml/nocf_noorth.png" class="post_img" alt="A blue histogram of treatment estimation errors centered on 25, ranging from about 20 to 30"></img>
        
         If our estimate is good, we would expect the normalized difference between our estimate of $\theta$ and the real theta to
         be centered on 0.

        This histogram shows that is not the case. Our estimate is way off and centered on a positive value. What went wrong?
        
        At a high level, part of what went wrong is that we did not explicitly model the effect of X on T. That influence is biasing our estimate. 
        Illustrating this explicitly is where our partially linear data generating process comes in handy. We can write out an equation for the error in our estimate.
        As a reminder, the goal here would be for the left hand side to converge to 0 as we get more data.

         <h4>regularization bias</h4>

        \( \sqrt{n}(\hat{\theta_0} - \theta_0) = \) <span style="color:#9b6981">\((\frac{1}{n}\sum_{i\in I}^nT_{i}^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nT_{i}U_{i}\)</span> \(+\) <span style="color:#7887a4">\((\frac{1}{n}\sum_{i\in I}^nT_{i}^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nT_{i}(g_0(X_i) - \hat{g_0}(X_i))\)</span>
        
        <ul>
            <li>The left hand side is our scaled error term - what we want to go to 0</li>
            <li style="color:#9b6981">Noise cancels out on average, so this term is a very small number, divided by a big number. Essentially 0</li>
            <li style="color:#7887a4">This term is where the problem is. Our estimate error is never going to be 0, and won;t quite be cenmtered on 0. This because of the deal we make as 
                data scientists working with complex data. Reduce the varaince (overfitting) of our machine learning model, we induce some bias in our estimate (often through 
                regularization). Additionally, T depends on X, and therefore also will not converge to 0. Because of this, g-g_hat times T will be small, but not zero. 
                It will be divided by a large number, and will converge to 0 eventually, but too slowly to be practical. </li>
        </ul>
        
        We have to remove the effect of X on T to circumvent this bias. We can do this in three steps:
        <ol>
            <li>Estimate \(T\) from \(X\) using ML model of choice (different from the direct method!)</li>
            <li> Estimate \(Y\) from \(X\) using ML model of choice</li>
            <li> Regress the residuals of each model onto eachother to get \(\theta_0\)</li>
        </ol>

        We can write out a new error equation like so:
        
        \(\sqrt{n}(\hat{\theta_0} - \theta_0) =\)<span style="color:#023743">\((EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nU_iV_i\)</span> \(+)\ <span style="color:#021743">\((EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^n(\hat{m_0}(X_i) - m_0(X_i))(\hat{g_0}(X_i) - g_0(X_i))\)</span> \(+ ...  \)
       
        <ul>
            <li>The left hand side is the same as before</li>
            <li>Noise cancels out on average, so this term is a very small number, divided by a big number. Essentially 0</li>
            <li>Now we have two small, non-0 numbers multiplied by eachother, divided by a large number. This will converge to 0 much more quickly than before </li>
            <li> ... this method adds a new term that we're going to ignore for now. But it comes back later!</li>
        </ul>
        
        In code, this new process only differs in the estimation \(g_0(X)\) amd \(\theta_0\). So fitting \(l_0(X)\) will be the same, but then we have:
        <pre><code class="python"> 
        # model for predicting T from X - new to the regularized version!
        ml_m = RandomForestRegressor()
        # model for predicting Y - T|X*theta from X
        ml_g = RandomForestRegressor()
        
        ml_m.fit(x,t)
        m_hat = ml_m.predict(x)

        # this is the part that's different
        v_hat = t - m_hat
        psi_a = -np.multiply(v_hat, v_hat)
        psi_b = np.multiply(v_hat, y - l_hat)
        theta_init = -np.mean(psi_b) / np.mean(psi_a)
            
        # get estimate for G
        ml_g.fit(x, y - t*theta_init)
        g_hat = ml_g.predict(x)
        </code>></pre>

        Similarly, when we get our finalt estmimate for \(\theta\)
        <pre><code class="python">    
         # compute residuals
         u_hat = y - g_hat
         # v_hat is the residuals from our m0 model
         psi_a = -np.multiply(v_hat, v_hat)
         psi_b = np.multiply(v_hat, u_hat)
            
         theta_hat = -np.mean(psi_b) / np.mean(psi_a)
         psi = psi_a * theta_hat + psi_b
         err = theta_hat - theta
         J = np.mean(psi_a)
         sigma2_hat = 1 / len(y) * np.mean(np.power(psi, 2)) / np.power(J, 2)
         err = err/np.sqrt(sigma2_hat)
        </code>></pre>

        If we plot a similar histogram over 200 simulations we'll get something like this:
        
         <img src="dml/nocf_orth.png" class="post_img" alt="A blue histogram of treatment estimation errors centered on -8, ranging from about -12 to -4"></img>

        And we have greatly reduced (but not eliminated) our bias!

        For this specific data generating process, we now have a way of estimating theta without regularization bias! However I mentioned earlier that we want to be 
        able to estimate more than only this process. In particular, step three involves linear regression, and only works in our partially linear example. How do
         we generalize the method of estimating theta?
         
         The least squares solution for linear relationships esentially finds the parameters for a line that minimizes the error between the predicted points on 
         the line, and the observed data. We can wrote this as the minimization of a cost function of our data and true parameters
         
         $$ \psi(W; \theta, \eta) = 0 $$
         
         This equation looks vary different but contains a lot of the same players as before:

         <ol>
             <li> \(W\) is the data (\(X\),\(Y\), and \(T\))</li>
             <li> \(\theta\) is the true treatment effect</li>
             <li> \(\psi\) is just some cost function. We are purposely not defining it because we want this to be a general solution, but you can think of it as any kind of 
                error minimization function</li>
             <li> \(\eta\) is called the nuissance parameter, and here contains g and m</li>
         </ol>

         Solving minimization problems like these are often difficult and subject to noise. To assure we find a robust solution, we're going to add one other condition
          to our equation (called a moment condition)
          
        $$ { \delta_{\eta}E[\psi(W; \theta, \eta][\eta - \eta_0] = 0} $$
         
         Technically, this is a direction al Gateaux derivative. Practically, what this means is that we expect that the true value of theta would be robust to 
         small purturbations in the nuissance parameters. This has the benefit of giving use estimates that will be stable in the presence of small changes to our
         ML models.
         
         There are whole branches of mathematics dedidated to solving these tpyes of equations with moment conditions, and there is no single good solution. All the 
         different solutions are called 'DML estimators'. Rather than getting into any specific estimator here, we're just going to trust that they exist, and move on.
         We'll talk a little bit about a specific estimator in the example code section.
         
         We now have a more generalizable set of steps
         <ol>
             <li> Estimate \(T\) from \(X\)</li>
             <li> Estimate \(Y\) from \(X\)</li>
             <li> Solve moment equation to get \(\theta\)</li>
         </ol>
        
         <h4>overfitting bias</h4>
        We now have a generalizable solution to regulatization bias. Additionally, with the definition of a cost function, we have a method of evaluating our DML
         estimator and comparing different models. Specifically, we can find the model that gives smallest value for our moment condition. The specific value of
         this function is usually called a 'score' or 'Neyman orthogonality score', and the closer to 0 it is the better. We will use this value to perform model 
        selection in the example later.
        
        But first, it's time to revisit out error equation in the partially linear case.
        
        $$ { \sqrt{n}(\hat{\theta_0} - \theta_0) = (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^nU_iV_i + (EV^2)^{-1}\frac{1}{\sqrt{n}}\sum_{i \in I}^n(\hat{m_0}(X_i) - m_0(X_i))(\hat{g_0}(X_i) - g_0(X_i)) + \frac{1}{\sqrt{n}}\sum_{i \in I}^{n}V_i(\hat{g_0}(X_i) - g_0(X_i)) } $$
        
        We've discussed the first terms perviously, but I'm revealing the last term we had hidden previously.
        * If any overfitting is present in \(\hat{g_0}\), the estimate will pick up some noise from the noise term. This will slow down the convergence of this new term to 0
        
        The solution to this bias is to fit \(g\) and \(m\) on a different set of data than the set used to estimate \(\theta\). Similar to how cross-validation avoids overfitting 
        during parameter selection, this method (called cross-fitting) avoids overfitting in out estimation of \(\theta\). This changes our DML steps slightly.
        
        <ol>
            <li>Split the data into \(K\) folds. For each fold:</li>
            <li> Estimate \(T\) from \(X\) using ML model of choice and fold \(K\)</li>
            <li> Estimate \(Y\) from \(X\) using ML model of choice and fold \(K\)</li>
            <li> Solve moment equation get \(\theta\) using other sets of data</li>
            <li>Select \(\theta\) estimate that gives the best solution over all splits.</li>
        </ol>

        In code, all this does is add a loop over folds:

        <pre><code class="python">
        from sklearn.model_selection import KFold
        # number of splits for cross fitting
        nSplit = 2
        x, y, t = get_data(n, n_x, theta)

        # cross fit
        kf = KFold(n_splits=nSplit)
        # save theta hats, and some variables for getting variance in theta_hat
        theta_hats = []
        sigmas = []
            for train_index, test_index in kf.split(x):
                x_train, x_test = x[train_index], x[test_index]
                y_train, y_test = y[train_index], y[test_index]
                t_train, t_test = t[train_index], t[test_index]

                ml_l = RandomForestRegressor()
                ml_m = RandomForestRegressor()
                ml_g = RandomForestRegressor()

                ml_l.fit(x_train,y_train)
                ml_m.fit(x_train,t_train)
                l_hat = ml_l.predict(x_test)
                m_hat = ml_m.predict(x_test)

                # initial guess for theta
                u_hat = y_test - l_hat
                v_hat = t_test - m_hat
                psi_a = -np.multiply(v_hat, v_hat)
                psi_b = np.multiply(v_hat, u_hat)
                theta_init = -np.mean(psi_b) / np.mean(psi_a)

                # get estimate for G
                ml_g.fit(x_train, y_train - t_train*theta_init)
                g_hat = ml_g.predict(x_test)

                # compute residuals
                u_hat = y_test - g_hat

                psi_a = -np.multiply(v_hat, v_hat)
                psi_b = np.multiply(v_hat, u_hat)

                theta_hat = -np.mean(psi_b) / np.mean(psi_a)
                theta_hats.append(theta_hat)
                psi = psi_a * theta_hat + psi_b
                sigma2_hat = 1 / len(y_test) * np.mean(np.power(psi, 2)) / np.power(J, 2)
                sigmas.append(sigma2_hat)
            err = np.mean(theta_hat) - theta
            err = err/np.sqrt(np.mean(sigmas))
        </code></pre>

        Using this process, we can correct the bias in our estimation

        <img src="dml/cf_orth.png" class="post_img" alt="A blue histogram of treatment estimation errors centered on 0.05, ranging from about -0.3 to 0.4"></img>
        
        Now we have a pretty good estimate! So far, we've gone over what the DML method is, and how is overcomes biases from regularization and overfitting to get good estimates of $\theta$ without making strong assumptions about the form of the data. We didn't go over here, but it is also possible to calculate confidence intervals for these estimates, which are neccessary for applications to clinical contexts. Now I'll show you how to apply it.
         
        
        <h3>Other resources</h3>
        <ul>
            <li href="https://towardsdatascience.com/double-machine-learning-for-causal-inference-78e0c6111f9d">This blog post on DML</li>
            <li>I liked <span hreaf="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://scholar.princeton.edu/sites/default/files/bstewart/files/felton.chern_.slides.20190318.pdf">these slides</span> so much I risked the social humilation that comes with liking a twitter post from years ago to show my support</li>
            <li href="https://docs.doubleml.org/stable/guide/basics.html#overcoming-regularization-bias-by-orthogonalization">DoubleML github (see this for code to exactly replicate the figures in the paper)</li>
        </ul>
        
        <h3>References</h3>
        <ul>
            <li>Rose, S., & Rizopoulos, D. (2020). Machine learning for causal inference in Biostatistics. In Biostatistics (Oxford, England) (Vol. 21, Issue 2, pp. 336–338). NLM (Medline). https://doi.org/10.1093/biostatistics/kxz045</li>
            <li>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2016). Double/Debiased Machine Learning for Treatment and Causal Parameters. http://arxiv.org/abs/1608.00060</li>
        </ul>
       
        </p>
    </div>

    <!--div class="card">
      <h2>How many roads must a random walker walk down before it gets out of Reykjavik?</h2>
      <h5>Analyzing your Google location history and biased random walkers on street graphs (in Python), July 25, 2022</h5>
      <img src="map/iceland_road.jpg" class="title_img" alt="A photograph of a road in Iceland. The road stretched off into rolling green hills. A glacier can be seen in the distance"></img>
      <h3>Google location data as introspection</h3>
        I spent a good part of my PhD learning about how people explore spaces. I specifically was interested in abstract spaces, where distances in these spaces map on to the similarity of some set of items. These items could be emotions, objects, school subjects, birds, or anything else you want. It turns out lots of 
        the theories how how humans learn and explore abstract spaces also hold water when examining how we or other animals explore physical spaces. These cross domain theories felt very satisfying to me; it seemed like they might be revealing something important about our behavior. 

        I wanted to take some of my knowledge of exploration, as well penchant for leanring new data vis methods, to learn something about my own exploration style through physical space. What sorts of cost functions might I have in mind when traveling? How well have I really explored my city? How much am I driven by novelty, or familiarity?
        This blog post will answer none of these questions. But it will go through the steps I took to learn things fundamental to answering these questions: the structure of Google maps location data, how to analyze road networks and implement random walks in Python. Along the way, we'll answer a much simpler and more pointless question: does my vacation to Iceland bear any qualitative similarity toa biased random walker?
      
        <h4>Getting your Google location data</h4>
        I'm going to use my own Google location data to visualize my path through Iceland. I followed these steps to download the data.

        The data comes with two types of files, a more information rich folder called XX, and a sparser .csv called XX. We're working with the .csv here. The important features of this .csv are latitudes and longitudes, as well as time stamps. 
        Coordinates will be mapped to publically available road networks using the package OSMNX, and timestapm will be used to order travel and filter data.

        We can get rid of all the extra columns and clean up the ones we want like so:
        
        <pre><code class="python">
        import pandas as pd
        import json

        # load
        with open('YOUR_PATH_TO_DATA/Takeout/Location History/Records.json') as data_file:    
            data = json.load(data_file)
        df = pd.json_normalize(data, 'locations')
       
        # get only relevant variables
        df = df[['latitudeE7', 'longitudeE7', 'timestamp']]
        
        # tranform them to be useful
        df.timestamp = pd.to_datetime(df.timestamp)
        df = df.assign(
            lat=df['latitudeE7']/1e7,
            long=df['longitudeE7']/1e7,
            year=df['timestamp'].dt.year,
            month=df['timestamp'].dt.month,
            day=df['timestamp'].dt.day,
            dow=df['timestamp'].dt.day_name(),
            time=df['timestamp'].dt.time,
        )

        </code></pre>

        As a quick validity check, let's use the OSMNX package to visualize all our different coordinates. Google has data on me since 2012, so this plot should include most of travels over the past decade.

        <pre><code class="python">
        import geopandas as gpd
        from shapely.geometry import Point
        import matplotlib.pyplot as plt

        # set up data structure of coordinates
        geo = [Point(xy) for xy in zip(df.long, df.lat)]
        gdf = gpd.GeoDataFrame(df, geometry=geo)

        # plot
        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
        gdf.plot(ax=world.plot(figsize=(14,14), color='lightgrey'), color="#7397CB")
        </code></pre>
        <img src="map/all_map.png" class="post_img" alt="A map of the world scattered with dark blue dots. Most dots are concentrated on the costs of the US, though some dots are seen in Australia, Morocco, Sri Lanka, Iceland, DUbai, and other countries."></img>

        Looks right to me! Now we're going filter down to the specific week where I was in Iceland
        <pre><code class="python">
        st =  '2017-07-31'
        en = '2017-08-07'
        iceland = df[(df['timestamp'] >= st) & (df['timestamp'] <= en)]

        # remove duplicate lat/long pairs
        iceland = iceland.drop_duplicates(subset=['lat', 'long', 'day', 'month'])
        </code></pre>

        Now that we have the data we'll be working with, we're ready to move on to working with road networks.
      
        <h3>Visualizing your paths through road networks</h3>
        We'll be using two packages to help visualize my path through Iceland: OSMNX, and networkX. OSMNX has data about road maps for most of the world. networkX allows us to use intuitive graph data structures to get information about the road networks. Both of these packages leverage some ideas and terminology from graph theory that we're going to go over before starting.
        
        A network is a representation of system of two or more separable units (caled nodes) and the interactions between them (called edges). Visually, nodes are often depicted as circles, with edges between them depicted as lines. In our case, edges are going to be roads, and nodes are any intersections or decision points in those roads.
        
        First, we're going to get a graph representation of Iceland's roads
        <pre><code class="python">
        import osmnx as ox

        ox.config(log_console=True, use_cache=True)
        # location where you want to find your route
        place     = 'Iceland'
        # find shortest route based on the mode of travel
        mode      = 'walk'        # 'drive', 'bike', 'walk'
        # find shortest path based on distance or time
        optimizer = 'length'        # 'length','time'
        # create graph from OSM within the boundaries of some 
        # geocodable place(s)
        graph = ox.graph_from_place(place, network_type = mode)
        </code></pre>

        We can see this graph has nodes and edges that we can access with networkX

        <pre><code class="python">
        import networkx as nx
        </code></pre>
        Our strategy for plotting my actual trajectory on this graph is as follows: get a starting coordinate; snap it to the closest spot on the road network; repeat for the next coordinate; get the shorted path between the start and end coordinate. We can then repeat this for every corrdinate to get one long path through Iceland.
        
        We can code up this strategy as follows:
        <pre><code class="python">
        route = []
        start_latlng = (iceland['lat'].values[0],iceland['long'].values[0])
        # find the nearest node to the start location
        orig_node = ox.get_nearest_node(graph, start_latlng)
        for i in range(len(iceland) - 1):
            # define the start and end locations in latlng
            end_latlng = (iceland['lat'].values[i+1],iceland['long'].values[i+1])
            # find the nearest node to the end location
            dest_node = ox.get_nearest_node(graph, end_latlng)
            #  find the shortest path
            route.extend(nx.shortest_path(graph,
                                    orig_node,
                                    dest_node,
                                    weight=optimizer))
            # advance to the next step
            orig_node = dest_node
        </code></pre>

        And then plot it
        <pre><code class="python">
        from itertools import groupby 

        # remove any dup;licates from the route - these cause the shortest path algorithm to break
        route = [i[0] for i in groupby(route)]
        fig, ax = ox.plot_graph_route(graph,
                                        route,
                                        node_size=0,
                                        edge_linewidth=2,
                                        edge_color='grey',
                                        route_color="#EB8D43",
                                        route_alpha=0.9,
                                        orig_dest_size=0)
        </code></pre>
        <img src="map/my_path.png" class="post_img" alt="A black background, with the all the roads in iceland plotted in dark grey. An orange line traces my path through the country."></img>

        And here we have a nice visiualization of my vacation. While I did travel through most parts of the country, the path I took is very directed. No one area has dense coverage and the path mostly sticks to the coast.
        
        These observations make sense given what I wanted to get out of the trip. I wanted to see as much of the country as possible in the week I was there. I also had prebooked accomodations in different cities for every night, which didn't allow me to wander or dwell in any one area. 

        How could this have looked different? What if I had showed up to Iceland and just wandered the streeets of Reykjavik, turning randomly whenever I felt like it? What I turned mostly randomly, but had a few goals in mind that I was trying to optimize? What if I wandered randomly, but then at somepoint drove to new, further away location so that I could wander there?
        <h3>Biased random walkers</h3>
        All of these strategies can coded up by imagining some agent who stands on a given node, and iteratively 'walks' to other connected nodes based on some rules. The rules are called 'biases', and often times even simple biases can sometimes lead to agents with behaviors that closely mimic real world behaviors. Even when they don't, they can identify how much of the variance in a behavior is explained by simple rules and help better identify which features we don't yet understand. We'll go over a few examples below

        <h4>Levy flights and novelty prioritization</h4>
        One ocool algorithm for generating walks is called a Levy flight. It is characterized by walkers who stay mostly in the local neighborhood, but occasionally make big jumps to far away nodes on the graph. These walks model animal foraging behavior, as well as the wikipedia exploration of some people.
        Practically, this is accomplished by haveing a random walker with a varying step size. The step size is drawn from an expoenetially decaying distribution, such that most steps will be small, but a few will be larger.

        Coding the Levy flight relies on two parameters, one that determines the steepenss of the exponential distribution (\(c\)) and one that determines the maximum step size (\(d)\)
        <pre><code class="python">
        # get decaying distribution of step sizes
        c = 1 # controls steepness of decay
        d = 5000 # max step size
        n = len(iceland)
        x = np.arange(1, d+1, 1)  # set upper bound for step size
        pdfLevy = np.power(1/x, c)  # set u value, where 1 <= c <= 3
        pdfLevy = pdfLevy/sum(pdfLevy)
        optimizer = 'length'
        mode = 'walk'
        
        # intialize
        start_latlng = (iceland['lat'].values[i],iceland['long'].values[i])
        orig_node = ox.get_nearest_node(graph, start_latlng)
        levy_route = [orig_node]
        </code></pre>

        We can then use these parameters to get a walk through the streets of Iceland similarly to what we did before. However, now instead of getting the successive nodes from the Google location data, we'll be getting them from the graph of Iceland itself.
        <pre><code clas="python">
        
        </code></pre>
        <img src="map/levy.png" class="post_img" alt="A black background, with the all the roads in iceland plotted in dark grey. A gold line traces a Levy flight through the country."></img>
        This algorithm looks a lot different from my path! It doesn't see the whole country, but the parts it does see it explores much more densely. Maybe part of the reason why this walk looks so different is that it tends to get stuck in cities or other densely connected areas. It might visit these streeets many times before it happens to stumble out.

        We can add biases to this walk algorithm to make it less likely to revisit nodes that its already been too. This is accomplished by adding an additional parameter (\(r\)) that sets the relative probability of revisiting nodes already in the path versus visiting new ones.
        <pre><code clas="python">
        
        </code></pre>
    </div>
    
 <!---   <div class="card">
      <h2>TITLE HEADING</h2>
      <h5>Title description, Sep 2, 2017</h5>
      <div class="fakeimg" style="height:200px;">Image</div>
      <h1>Example, with code</h1>
        We're now going to walk through an example application of DML to a simulated medical dataset. If you didn't read the above section, here's a reminder of the steps of DML. 

        For simplicity, we're going to use a similar data generating process we discuessed above, but with one change. Now, \(\theta\) is going to be a function of \(X\). This means that treatment effect will differ for different individuals. This is expected in medical contextx - for example, people with high body mass (X) tend to reach lower enesthetics depths (Y) for the same dose of an anesthetic (T). When \(\theta\) is a function of \(X\), it is called a heterogeneous, of conditional average treatment effect (abbreviated CATE in the package nomenclature).
        
        $$ {Y = T\theta_{0}(X) + g_{0}(X) + U, E[U | X,T] = 0]} $$
        $$ {T = m_{0}(X) + V, E[V | X] = 0} $$
        
        As a reminder, \(X\) are our patient features, \(Y\) is an predictor of outcome or risk, and \(T\) is an indicator of treatment. While in theory, \(T\) can be binary, continuous, or categorical, the specific code we are discussing here does not work for categorical treatments. \(\theta_0\) is the conditional average treatment effect, the term we're interested in. We can simulate this data in python using the same code as before. As a reminder:
        
        <pre><code class="python"> simulating data code </code>></pre>
        
        We're also going to split our data into folds for evaluation. It's important to remember that each training fold is going to be further split within the DML estimator, so we want to make sure there are enough observations in the fold to split further.
        
        <pre><code class="python"> data splitting code </code>></pre>
        
        The econML package has different 'estimator' functions that all take data, a Y model (\(g_0(X)\)), and a T model (\(m_0(X)\)) as input. Therefore, we're going to start with step 2 listed above: estimating \(T\) (we'll do the data splitting process later). You can really do this however you want, we're just going to define a simple random forest model here. econML has the benefit of working with GridSearchCV object as well, so we're actually going to define not one speciifc model, but a set models with different parameterizations (if you aren't familiar with GridSearchCV, check out its docs).
        
        <pre><code class="python">
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import GridSearchCV
        
        # parameters for forest
        params = {
            'max_depth': [5, 10],
            'min_samples_leaf': [2, 4],
            'min_samples_split': [2, 4],
            'class_weight': ['balanced', None],
            'n_estimators':[400, 1000]
            }
        
        t_mdls = GridSearchCV(RandomForestRegressor(),
                                params,
                                cv=nFold,
                                verbose=1,
                                scoring='f1_macro')
        t_mdl = t_mdls.fit(X_train, y_train).best_estimator_ 
        </code>></pre>

        You can evaluate these models any way you want to. We'll use ROC
        <pre><code class="python"> 
        from sklearn.metrics import roc_auc_score, roc_curve
        import matplotlib.pyplot as plt
        
        plt.figure()
        pred = t_mdl.predict_proba(X_test)[:,1]
            
        fpr, tpr, _ = roc_curve(y_test,pred,drop_intermediate=False)
        roc_auc = roc_auc_score(y_test,pred)

        plt.plot(fpr, tpr,
                lw=lw, label='Risk: %0.2f' % roc_auc)
        plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Outcome ROC Curves')
        plt.legend(loc="best")
        plt.show() 
        </code>></pre>
        
        <div class="fakeimg">Image of graph</div>

        We can do exactly the same thing for the \(Y\) model. Let's use a gradient boosted classifier to mix things up.
        
        <pre><code class="python"> 
        from xgboost import XGBClassifier
        # parameters for XGB
        params = {
            'max_depth': range (2, 10, 1),
            'n_estimators': range(60, 220, 40),
            'learning_rate': [0.1, 0.01, 0.05]
            }
        y_mdls = GridSearchCV(XGBClassifier(use_label_encoder=False,eval_metric='logloss', n_jobs=1),
                                params,cv=nFold,n_jobs=nJobs,verbose=1,scoring = 'f1_macro')
        
        y_mdl = y_mdls.fit(X_train_clean, np.ravel(y_train)).best_estimator_
        </code>></pre>

        <pre><code class="python"> 
        plt.figure()

        pred = y_mdl.predict_proba(X_test)[:,1]
            
        fpr, tpr, _ = roc_curve(y_test,pred,drop_intermediate=False)
        roc_auc = roc_auc_score(y_test,pred)

        plt.plot(fpr, tpr,
                lw=lw, label='Risk: %0.2f' % roc_auc)
        plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Outcome ROC Curves')
        plt.legend(loc="best")
        plt.show() 
        </code>></pre>
        <div class="fakeimg">Image of graph</div>
        
        We can now pick an estimator. econML has 3 main estimators that provide confidence intervals. SparseLinear and Linear estimator will only work if you have many more observations than variables (see this link for comparisons of different estimators). For my data, this was not the case, so left me with 1 option: CausalForest estimator (REF). Like random forest models, this estimator also has the benefit of being able to estimate non-linear treatment effects in a piece-wise fashion.
        
        <pre><code class="python"> 
        from econml.dml import CausalForestDML
        
        est = CausalForestDML(model_y=y_mdl, model_t=t_mdl, discrete_treatment=True, cv=5)
        est.fit(Y=y_train, T=T_train, X=X_train) 
        </code>></pre>
        
        This function takes care of the cross-fitting procedure! The argument cv defines how many folds to use for cross fitting. The default is 3, but the original paper recommends using 5 or 6 if possible.
        
        Also similar to RandomForestCLassifiers in scikit learn, the CausalForest estimator has many other parameters. If you're familiar with random forests many of these parameters will be familiar: the number of trees to include, the maximum depth of those trees, etc. The big exception in parameters between causal forests in econML and sklearn is that econML forest has no class weightnig option (XXX). This is because the causal forest method makes use of a specific weighting strategy already (REF). 
        
        Additionally, the econML estimator does not take the 'params' argument, and therefore can't be used as input into sklearns Grid or RandomSearchCV functions. However we can do our own hyperparameter selection by comparing the 'score' between different parameter choices and selecting the lowest one. For brevity, we're going to use a random sampling strategy for hyperparameter selection. This means that rather than trying every possible parameter combination, we're going to randomly select some and pick the best option, assuming that we will get similar performance in the long run
        
        <pre><code class="python"> 
        import pandas as pd
        from sklearn.model_selection import KFold
        
        # save the scores so we can plot them
        no_score = pd.DataFrame(columns=['depth', 'min_leaf', 'min_split', 'n', 'score'])
        # parameters for causal forest
        est_params = {
            'max_depth': [5, 10, None],
            'min_samples_leaf': [5, 2],
            'min_samples_split': [10, 4],
            'n_estimators': [100, 500]
        }
        n_iter = 20  # number of random parameter sets
        skf = StratifiedKFold(n_splits=nFold)
        </code></pre>
        
        Now that we how our cross validation and parameters set up, we can find the best parameters. We'll first pick a random starting set, and specify a model
        
        <pre><code class="python">        
        # randomly pick some parameters
        n = np.random.choice(est_params['n_estimators'])
        d = np.random.choice(est_params['max_depth'])
        l = np.random.choice(est_params['min_samples_leaf'])
        s = np.random.choice(est_params['min_samples_split'])

        # get the cate estimate
        est = CausalForestDML(model_y=y_mdl,
                            model_t=t_mdl,
                            discrete_treatment=True, 
                            verbose=0, 
                            max_depth=d,
                            min_samples_leaf=l,
                            min_samples_split=s,
                            n_estimators=n)
        </code>></pre>
        
        Now we can evaluate this parameter selection across all folds
        <pre><code class="python">
        # stratified cv
        for train_index, test_index in skf.split(x, y):
            X_htrain, X_htest = x[train_index], x[test_index]
            y_htrain, y_htest = y[train_index], y[test_index]
            T_htrain, T_htest = t[train_index], t[test_index]

            est.fit(Y=np.ravel(y_htrain), T=T_htrain, X=X_htrain)
            scores.append(est.score(y_htest, T_htest, X=X_htest))

            curr_dict = {'depth': [d],
                        'min_leaf': [l],
                        'min_split': [s],
                        'n': [n],
                        'score': [np.mean(scores)],
                        'score_sc': [np.std(scores)],
                        'med': [med]}
            no_score = pd.concat([hp_score, curr_dict])
        </code>></pre>
        
        If you repeat this process and save all the scores, you can then pick the best parameter set

        <pre><code class="python">
        # pick the best parameters, and fit the model
        best_n = no_score.loc[no_score['score'] == no_score['score'].min(), 'n'].values[0]
        best_d = no_score.loc[no_score['score'] == no_score['score'].min(), 'depth'].values[0]
        best_msl = no_score.loc[no_score['score'] == no_score['score'].min(), 'min_leaf'].values[0]
        best_mss = no_score.loc[no_score['score'] == no_score['score'].min(), 'min_split'].values[0]
        best_est = CausalForestDML(model_y=y_mdl,
                                    model_t=t_mdl,
                                    verbose=0, cv=5,
                                    max_depth=best_d,
                                    min_samples_leaf=best_msl,
                                    min_samples_split=best_mss,
                                    n_estimators=best_n)
        best_est.fit(np.ravel(y_train), T_train, X=X_train)
        </code>></pre>
        
        The 'score' function (if you use the default option) gives you the mean squared error for the last step of the DML algorithm: regressing the residuals of your two machine learning models against eachother. It uses this error as a proxy for error in estimating $$\theta$$.
        
        <pre><code class="python"> 
        import seaborn as sns
        
        # print scores for each medicine
        no_score['param_set'] = ["_".join(x) for x in hp_score[['n', 'depth', 'min_leaf', 'min_split']].astype(str).values]
        sns.stripplot(data=no_score,  y="score", hue="param_set", size=10)
        plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")
        </code>></pre>
        <div class="fakeimg">Image of graph</div>
        
        We can now fit the model using the parameters that gave the lowest score
        
        <pre><code class="python"> 
        dml = CausalForestDML(model_y=y_mdl,
                            model_t=t_mdl,
                            verbose=0, cv=5,
                            max_depth=best_d,
                            min_samples_leaf=best_msl,
                            min_samples_split=best_mss,
                            n_estimators=best_n)
        </code>></pre>
        
        We can now look at the distribution for of CATE's for all individuals in the test set.
        
        <pre><code class="python"> 
        # plot
        n_bins = 10
        effect = est.effect(X=X_test_clean)
        plt.hist(effect, bins = n_bins, label=med,alpha=.8, color=colors[med_idx])
        plt.title('Effect Values')
        plt.xlabel('Effect')
        plt.ylabel('Count')    
        </code>></pre>
        <div class="fakeimg">Image of graph</div>

        We can also look at the CI for $$\theta$$ in a single individual.

        <pre><code class="python"> 
        # get individual CATES
        patient_idx = np.random.randint(np.shape(X_test)[0])
                
        # get cate
        cate = mdl.effect(X_test[patient_idx:patient_idx+1,])[0]
        
        # get cate CI
        lb, ub = mdl_dict[name].effect_interval(X_test_clean[patient_idx:patient_idx+1,], alpha=0.05)
        
        # plot CATEs with CI for individual patients
        plt.figure(figsize=(8,6))
        plt.errorbar(1, cate, yerr=ci, 
                        fmt="o", ecolor='k', zorder=1)
        plt.tight_layout()
        </code>></pre>
        <div class="fakeimg">Image of graph</div>

        How'd we do? Since this is simulated data, we can see how well our estimated treament matches the true effect

        <pre><code class="python"> 
        # plot   
        </code>></pre>

        We did pretty well! As expected, given the simulated data. This concludes the extent of the code walkthrough. If you're still looking for more, check out the other resources I found useful below.
        
    </div>
  </div>
  <div class="rightcolumn">
    <div class="card">
      <h3>Popular Post</h3>
      <div class="fakeimg">Image</div><br>
      <div class="fakeimg">Image</div><br>
      <div class="fakeimg">Image</div>
    </div>
    
  </div>
--->
</div>

